{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3cb4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007db6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba5d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Similarity Functions ----------\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set, b_set = set(a.split()), set(b.split())\n",
    "    if not a_set or not b_set:\n",
    "        return 0\n",
    "    return len(a_set & b_set) / len(a_set | b_set)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    vectorizer = CountVectorizer().fit([a, b])\n",
    "    vectors = vectorizer.transform([a, b])\n",
    "    return cosine_similarity(vectors)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e818508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main Rule-Based Scoring ----------\n",
    "def classify_response(response, correct_answer):\n",
    "    # Ensure both are strings\n",
    "    response = str(response).strip().lower()\n",
    "    correct_answer = str(correct_answer).strip().lower()\n",
    "\n",
    "    if response == \"\" or response in [\"idk\", \"i don't know\", \"i dunno\"]:\n",
    "        return -1\n",
    "\n",
    "    fuzzy_ratio = fuzz.token_sort_ratio(response, correct_answer) / 100\n",
    "    jaccard = jaccard_similarity(response, correct_answer)\n",
    "    cosine = cosine_sim(response, correct_answer)\n",
    "\n",
    "    try:\n",
    "        semantic = nlp(response).similarity(nlp(correct_answer))\n",
    "    except Exception:\n",
    "        semantic = 0  # fallback\n",
    "\n",
    "    resp_nums = re.findall(r\"\\d+\", response)\n",
    "    corr_nums = re.findall(r\"\\d+\", correct_answer)\n",
    "    num_match = 1 if resp_nums and resp_nums == corr_nums else 0\n",
    "\n",
    "    # Weighted score\n",
    "    total_score = (\n",
    "        0.55 * fuzzy_ratio +\n",
    "        0.2 * jaccard +\n",
    "        0.15 * semantic +\n",
    "        0.05 * cosine +\n",
    "        0.05 * num_match\n",
    "    )\n",
    "\n",
    "    if total_score > 0.60:\n",
    "        return 1\n",
    "    elif total_score > 0.59:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4cc281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"test_cleaned.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a8cef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin Kam\\AppData\\Local\\Temp\\ipykernel_3288\\2825088402.py:15: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  semantic = nlp(response).similarity(nlp(correct_answer))\n"
     ]
    }
   ],
   "source": [
    "# Apply classification\n",
    "df[\"Predicted_Label\"] = df.apply(\n",
    "    lambda row: classify_response(row[\"Response\"], row[\"CorrectAnswer\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0acb7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions: 23443/30466\n",
      "Accuracy: 76.95%\n"
     ]
    }
   ],
   "source": [
    "# ---------- Evaluation ----------\n",
    "if \"label\" in df.columns:\n",
    "    total_rows = len(df)\n",
    "    correct_predictions = (df[\"Predicted_Label\"] == df[\"label\"]).sum()\n",
    "    accuracy = correct_predictions / total_rows * 100\n",
    "\n",
    "    print(f\"Correct predictions: {correct_predictions}/{total_rows}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7fa5b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'test_responses_new.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save to a new CSV\n",
    "df.to_csv(\"test_responses_new.csv\", index=False)\n",
    "print(\"Results saved to 'test_responses_new.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
