{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from rapidfuzz import fuzz\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007db6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Similarity Functions ----------\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set, b_set = set(a.split()), set(b.split())\n",
    "    if not a_set or not b_set:\n",
    "        return 0\n",
    "    return len(a_set & b_set) / len(a_set | b_set)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    vectorizer = CountVectorizer().fit([a, b])\n",
    "    vectors = vectorizer.transform([a, b])\n",
    "    return cosine_similarity(vectors)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e818508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main Rule-Based Scoring ----------\n",
    "def classify_response(response, correct_answer, high_threshold, low_threshold):\n",
    "    response = str(response).strip().lower()\n",
    "    correct_answer = str(correct_answer).strip().lower()\n",
    "\n",
    "    if response == \"\" or response in [\"idk\", \"i don't know\", \"i dunno\"]:\n",
    "        return -1\n",
    "\n",
    "    fuzzy_ratio = fuzz.token_sort_ratio(response, correct_answer) / 100\n",
    "    jaccard = jaccard_similarity(response, correct_answer)\n",
    "    cosine = cosine_sim(response, correct_answer)\n",
    "\n",
    "    try:\n",
    "        semantic = nlp(response).similarity(nlp(correct_answer))\n",
    "    except Exception:\n",
    "        semantic = 0\n",
    "\n",
    "    resp_nums = re.findall(r\"\\d+\", response)\n",
    "    corr_nums = re.findall(r\"\\d+\", correct_answer)\n",
    "    num_match = 1 if resp_nums and resp_nums == corr_nums else 0\n",
    "\n",
    "    # Weighted total score\n",
    "    total_score = (\n",
    "        0.4 * fuzzy_ratio +\n",
    "        0.2 * jaccard +\n",
    "        0.2 * semantic +\n",
    "        0.15 * cosine +\n",
    "        0.05 * num_match\n",
    "    )\n",
    "\n",
    "    # Threshold-based classification\n",
    "    if total_score > high_threshold:\n",
    "        return 1\n",
    "    elif total_score > low_threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Main Loop ----------\n",
    "df = pd.read_csv(\"train_cleaned.csv\", encoding=\"latin1\")\n",
    "results = []\n",
    "\n",
    "for high_threshold in [round(x, 2) for x in np.arange(0.60, 0.651, 0.01)]:\n",
    "    for low_threshold in [round(x, 2) for x in np.arange(0.55, high_threshold, 0.01)]:\n",
    "        temp_df = df.copy()\n",
    "\n",
    "        temp_df[\"Predicted_Label\"] = temp_df.apply(\n",
    "            lambda row: classify_response(row[\"Response\"], row[\"CorrectAnswer\"], high_threshold, low_threshold),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Evaluate accuracy\n",
    "        if \"label\" in temp_df.columns:\n",
    "            total_rows = len(temp_df)\n",
    "            correct_predictions = (temp_df[\"Predicted_Label\"] == temp_df[\"label\"]).sum()\n",
    "            accuracy = correct_predictions / total_rows * 100\n",
    "        else:\n",
    "            accuracy = 0\n",
    "\n",
    "        print(f\"Tested: high={high_threshold}, low={low_threshold} ‚Üí Accuracy={accuracy:.2f}%\")\n",
    "\n",
    "        results.append({\n",
    "            \"result_1_limit\": high_threshold,\n",
    "            \"result_0_limit\": low_threshold,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "        temp_df.to_csv(\"train_responses.csv\", index=False)\n",
    "\n",
    "        # Delete temp file\n",
    "        if os.path.exists(\"train_responses.csv\"):\n",
    "            os.remove(\"train_responses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0acb7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save Final Summary ----------\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"threshold_results_train.csv\", index=False)\n",
    "print(\"\\n‚úÖ All threshold combinations tested.\")\n",
    "print(\"üìÑ Results saved to 'threshold_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab89de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Print Best Thresholds ----------\n",
    "best = results_df.loc[results_df['accuracy'].idxmax()]\n",
    "print(f\"\\nüèÜ Best thresholds found:\")\n",
    "print(f\"Result=1 limit: {best['result_1_limit']}\")\n",
    "print(f\"Result=0 limit: {best['result_0_limit']}\")\n",
    "print(f\"Accuracy: {best['accuracy']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
